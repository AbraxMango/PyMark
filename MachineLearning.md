# W

## Dataset

- sklearn.datasetsï¼šåŠ è½½è·å–æµè¡Œæ•°æ®é›†
- sklearn.datasets.load_*()ï¼šè·å–å°è§„æ¨¡æ•°æ®é›†ï¼Œæ•°æ®åŒ…å«åœ¨datasetsé‡Œï¼Œâ€™*â€™ä¸ºæ•°æ®é›†åç§°
- sklearn.datasets.fetch_*(data_home=None, subset=â€™â€™)ï¼šè·å–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œéœ€è¦ä»ç½‘ä¸Šä¸‹è½½ï¼Œdata_homeè¡¨ç¤ºæ•°æ®é›†ä¸‹è½½çš„ç›®å½•ï¼Œsubsetè¡¨ç¤ºåœ¨æŸäº›æ•°æ®é›†ä¸­ï¼Œå¯ä»¥æŒ‡å®šå­é›†çš„ç±»å‹ï¼Œä¾‹å¦‚ â€™trainâ€™ æˆ– â€™testâ€™

```python
# é¸¢å°¾èŠ±
iris = load_iris()
print(iris)
print(type(iris))  # typeä¸ºdatasets.base.Bunch(å­—å…¸æ ¼å¼)
```

### é”®å€¼å¯¹

- dataï¼šç‰¹å¾æ•°æ®æ•°ç»„ï¼Œæ˜¯[n_sample*n_features]äºŒç»´numpy.ndarrayæ•°ç»„
- targetï¼šæ ‡ç­¾æ•°ç»„ï¼Œæ˜¯n_samplesçš„ä¸€ç»´æ•°ç»„
- DESCRï¼šæ•°æ®æè¿°
- feature_namesï¼šç‰¹å¾æ•°æ®ï¼Œæ‰‹å†™æ•°å­—ä»¥åŠå›å½’æ•°æ®é›†æ²¡æœ‰
- target_nameï¼šæ ‡ç­¾å

### æ•°æ®é›†åˆ’åˆ†

> ç”±äºå¯¹æ•°æ®è®­ç»ƒå®Œä¹‹åéœ€è¦æµ‹è¯•æ•°æ®ï¼Œæ‰€ä»¥éœ€è¦å¯¹æ•°æ®é›†è¿›è¡Œåˆ’åˆ†ï¼Œä»¥ä¿ç•™æ•°æ®ã€‚ä¸€èˆ¬è®­ç»ƒé›†å 70%~80%ï¼Œæµ‹è¯•é›†å 20%~30%

API

```python
sklearn.model_selection.train_test_split(arrays, *options)
```

å‚æ•°ï¼š

- arraysï¼šæ ·æœ¬æ•°ç»„ï¼ŒåŒ…å«ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾
- test_sizeï¼šæµ‹è¯•é›†å¤§å°ï¼Œå¯ä»¥ä¸ºæµ®ç‚¹æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰æˆ–æ•´æ•°ï¼ˆæ ·æœ¬æ•°ï¼‰
- train_sizeï¼šè®­ç»ƒé›†å¤§å°ï¼Œå¯ä»¥ä¸ºæµ®ç‚¹æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰æˆ–æ•´æ•°ï¼ˆæ ·æœ¬æ•°ï¼‰
- random_stateï¼šéšæœºæ•°ç§å­ï¼Œç”¨äºé‡å¤å®éªŒ
- shuffleï¼šæ˜¯å¦åœ¨åˆ†å‰²ä¹‹å‰å¯¹æ•°æ®è¿›è¡Œæ´—ç‰Œï¼Œé»˜è®¤ä¸ºTrue
- returnï¼šx_trainè®­ç»ƒé›†ç‰¹å¾å€¼ï¼Œx_testæµ‹è¯•é›†ç‰¹å¾å€¼ï¼Œy_trainè®­ç»ƒé›†ç›®æ ‡å€¼ï¼Œy_testæµ‹è¯•é›†ç›®æ ‡å€¼

```python
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22)
```

## ç‰¹å¾å·¥ç¨‹

> æ•°æ®å’Œç‰¹å¾å†³å®šäº†æœºæ¢°å­¦ä¹ çš„ä¸Šé™ï¼Œè€Œæ¨¡å‹å’Œç®—æ³•åªæ˜¯é€¼è¿‘äº†è¿™ä¸ªä¸Šé™

### å­—å…¸ç‰¹å¾æå–ï¼ˆç‰¹å¾ç¦»æ•£åŒ–ï¼‰

- sklearn.feature_extraction.DictVectorizer(sparse=True, â€¦)
- DictVectorizer.fit_transform(X) Xï¼šå­—å…¸æˆ–åŒ…å«å­—å…¸çš„è¿­ä»£å™¨ï¼Œè¿”å›çš„æ˜¯sparse(ç¨€ç–)çŸ©é˜µ
- DictVectorizer.inverse_transform(X) Xï¼šarrayæ•°ç»„æˆ–è€…sparseçŸ©é˜µï¼Œè¿”å›è½¬æ¢ä¹‹å‰çš„æ•°æ®æ ¼å¼
- DictVectorizer.get_feature_names()ï¼šè¿”å›ç±»åˆ«åç§°

```python
from sklearn.feature_extraction import DictVectorizer

data_1 = [{"city": "åŒ—äº¬", "temperature": 100},
          {"city": "ä¸Šæµ·", "temperature": 60},
          {"city": "æ·±åœ³", "temperature": 30}]

# 1.å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
d_transfer = DictVectorizer(sparse=False)

# 2.è°ƒç”¨fit_transform()
data_new_1 = d_transfer.fit_transform(data_1)

# print("   ä¸Šæµ·  åŒ—äº¬  æ·±åœ³")
print(data_new_1)
```

<aside> ğŸ’¡

åº”ç”¨åœºæ™¯ï¼špclassï¼Œsexæ•°æ®é›†ä¸­ç±»åˆ«ç‰¹å¾æ¯”è¾ƒå¤šï¼›å…¬å¸ï¼ŒåŒäº‹åˆä½œ

</aside>

### æ–‡æœ¬ç‰¹å¾æå–

æ–¹æ³•ä¸€ï¼šCountVectorizer

- sklearn.feature_extraction.text.CountVectorizer(stop_words=[â€mambaâ€, â€œoutâ€, â€¦], â€¦) stop_words åœç”¨è¯ï¼Œç”¨åˆ—è¡¨ä¼ 
- CountVectorizer.fit_transform(X) Xï¼šæ–‡æœ¬æˆ–è€…åŒ…å«æ–‡æœ¬å­—ç¬¦ä¸²çš„å¯è¿­ä»£å¯¹è±¡ï¼Œè¿”å›sparseçŸ©é˜µ
- CountVectorizer.inverse_transform(X) Xï¼šarrayæ•°ç»„æˆ–è€…sparseçŸ©é˜µï¼Œè¿”å›è½¬æ¢ä¹‹å‰çš„æ•°æ®æ ¼å¼
- CountVectorizer.get_feature_names()ï¼šè¿”å›å•è¯åˆ—è¡¨

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

data_2_En = ["life is short, i like python very very much", "life is long, i dislike python"]
data_2_Cn = ["å¤§çƒŸæ† å˜´é‡Œ å¡, æˆ‘ åªæŠ½ ç¬¬äº”ä»£", "æ¯”å…»çš„ å¿«å°å°, æˆ‘ ç°åœ¨ è‚ºç—’ç—’"]  # ä¸­æ–‡éœ€è¦ç©ºæ ¼éš”å¼€æ‰èƒ½åˆ†ä¸ºè¯è€Œéå¥å­

# 1.å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
t_transfer = CountVectorizer()

# 2.è°ƒç”¨fit_transform
data_new_2_En = t_transfer.fit_transform(data_2_En)

print(t_transfer.get_feature_names_out())  # è·å–ç‰¹å¾åå­—
print(data_new_2_En.toarray())  # ç”¨toarray()æ–¹æ³•ä»¥è¾“å‡ºäºŒä½çŸ©é˜µ
data_new_2_Cn = t_transfer.fit_transform(data_2_Cn)
print(t_transfer.get_feature_names_out())
print(data_new_2_Cn.toarray())

# ä¸­æ–‡æ–‡æœ¬ç‰¹å¾æå–,  å€ŸåŠ©jiebaè‡ªåŠ¨åˆ†è¯
import jieba

data_3 = ["æˆ‘çš„é”åˆ»å°±åƒä¸€æ ¹æ ¹ç³–, å£å‘³ä¸°å¯Œåˆ°éš¾ä»¥æƒ³è±¡",
          "åªè¦ä½ å¾€æˆ‘é¼»å˜´é‡Œæ”¾, æˆ‘è°ˆç¬‘é—´å¸å¹²å¤ªå¹³æ´‹",
          "æ ¸æ±¡æ°´ä¸è¦å†ä¹±æ’æ”¾, é¬¼å­æƒ³å–å°±è§£æˆ‘è£¤è£†",
          "å¤§å®¶è¦æ˜¯ä¹°ä¸åˆ°é”åˆ», å¾€æˆ‘çš„å˜´é‡Œå¸ï¼Œéƒ½ä¸€æ ·"]

fyy = []
for sen in data_3:
    fyy.append(" ".join(jieba.cut(sen)))  # è¿”å›çš„æ˜¯è¯è¯­ç”Ÿæˆå™¨

print(fyy)
print("--------------------")
# # 1.å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
t_transfer = CountVectorizer()

# 2.è°ƒç”¨fit_transform
data_new_3 = t_transfer.fit_transform(fyy)

print(t_transfer.get_feature_names_out())  # è·å–ç‰¹å¾åå­—
print(data_new_3.toarray())  # ç”¨toarray()æ–¹æ³•ä»¥è¾“å‡ºäºŒä½çŸ©é˜µ
```

æ–¹æ³•äºŒï¼šTfidfVectorizer

> Tf-idfæ–‡æœ¬ç‰¹å¾æå–ï¼Œå…³é”®è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†ï¼Œæˆ–ä¸€ä¸ªè¯­æ–™åº“çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚       è¯é¢‘ term frequency - tfï¼Œé€†å‘æ–‡æ¡£ç‡ inverse document frequency - idf ï¼šæ€»æ–‡ä»¶æ•°ç›®é™¤ä»¥åŒ…å«è¯¥è¯è¯­æ–‡ä»¶çš„æ•°ç›®ï¼Œå°†ç»“æœå–10ä¸ºåº•çš„å¯¹æ•°ã€‚é‡è¦ç¨‹åº¦ï¼štfidf(i, j) = tf(i, j) * idf(i)

```python
data_4 = ["åœ¨è¿™ä¹ˆå†·çš„å¤©, æƒ³æŠ½æ ¹ç”µå­çƒŸ", "å¯é”å…‹æ²¡æœ‰ç”µ, å¯æ˜¯é›ªè±¹å·²å¤±è”",
          "æ‘˜ä¸ä¸‹æˆ‘è™šä¼ªçš„å‡é¢, å‡ å¥èƒ¡è¨€è¢«å¥‰ä¸ºåœ£è°", "å°ä¸€å£ä½ è¡€æ±—çš„é¦™ç”œ, å¯æ˜¯é’±é£˜è¿›åŒçœ¼"]

distance = []
for sen in data_4:
    distance.append(" ".join(jieba.cut(sen)))

# 1.å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
Tf_transfer = TfidfVectorizer()

# 2.è°ƒç”¨fit_transform
data_new_4 = Tf_transfer.fit_transform(distance)
print(data_new_4.toarray())

# ----- å›¾åƒç‰¹å¾æå– ----- (æ·±åº¦å­¦ä¹ )

# ----- ç‰¹å¾é¢„å¤„ç† -----
# é€šè¿‡ä¸€äº›è½¬æ¢å‡½æ•°, å°†ç‰¹å¾æ•°æ®è½¬æ¢æˆæ›´åŠ é€‚åˆç®—æ³•æ¨¡å‹çš„ç‰¹å¾æ•°æ®è¿‡ç¨‹ : æ— é‡çº²åŒ–, ä¸åŒè§„æ ¼çš„æ•°æ®è½¬æ¢åˆ°åŒä¸€è§„æ ¼
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler, StandardScaler
```

### ç‰¹å¾é¢„å¤„ç†

> é€šè¿‡ä¸€äº›è½¬æ¢å‡½æ•°ï¼Œå°†ç‰¹å¾æ•°æ®è½¬æ¢æˆæ›´åŠ é€‚åˆç®—æ³•æ¨¡å‹çš„ç‰¹å¾æ•°æ®è¿‡ç¨‹ï¼šæ— é‡çº²åŒ–ï¼Œä¸åŒè§„æ ¼çš„æ•°æ®è½¬æ¢åˆ°åŒä¸€è§„æ ¼

```python
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler, StandardScaler
```

1. å½’ä¸€åŒ– MinMaxScaler

- å¥å£®æ€§è¾ƒå·®ï¼Œé€‚åˆä¼ ç»Ÿç²¾ç¡®å°æ•°æ®åœºæ™¯

```python
# x' = (x - min) / (max - min) , x'' = x' * (mx - mi) + mi
# max/min ä¸ºæ¯ä¸€åˆ—çš„æœ€å¤§/å°å€¼, x''ä¸ºæœ€ç»ˆç»“æœ. mxå’Œmiä¸ºæŒ‡å®šåŒºé—´å€¼, é»˜è®¤ä¸º1å’Œ0

# 1.è·å–æ•°æ®
lst = np.array([[40920, 8.326976, 0.953952, 3], [14488, 7.153469, 1.673904, 2], [26052, 1.441871, 0.805124, 1],
                [75136, 13.147394, 0.428964, 1], [38344, 1.669788, 0.134296, 1], [72993, 10.141740, 1.032955, 1],
                [35948, 6.830792, 1.213192, 3], [42666, 13.276369, 0.543880, 3], [67497, 8.631577, 0.749278, 1],
                [35483, 12.273169, 1.508053, 3], [50242, 3.723498, 0.831917, 1], [63275, 8.385879, 1.669485, 1],
                [5569, 4.875435, 0.728658, 2], [51052, 4.680098, 0.625224, 1], [77372, 15.299570, 0.331351, 1],
                [43673, 1.889461, 0.191283, 1], [61364, 7.516754, 1.269164, 1], [69673, 14.239195, 0.261333, 1],
                [15669, 0.000000, 1.250185, 2], [28488, 10.528555, 1.304844, 3], [6487, 3.540265, 0.822483, 2],
                [37708, 2.991551, 0.833920, 1], [22620, 5.297865, 0.638306, 2], [28782, 6.593803, 0.187108, 3],
                [19739, 2.816760, 1.686209, 2], [36788, 12.458258, 0.649617, 3], [5741, 0.000000, 1.656418, 2],
                [28567, 9.968648, 0.731232, 3], [6808, 1.364838, 0.640103, 2]]).reshape(29, 4)
data_5 = pd.DataFrame(lst,
                      index=list(f"{i}" for i in range(1, 30)),
                      columns=list(["milage", "Liters", "Consumtime", "target"])
                      )
# print(data_5)
data_5 = data_5.iloc[:, :3]

# 2.å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
pre_transfer = MinMaxScaler()  # feature_range=[]å‚æ•°ä¸ºæŒ‡å®šåŒºé—´, é»˜è®¤ä¸º[0,1]

# 3.è°ƒç”¨fit_transform
data_new_5 = pre_transfer.fit_transform(data_5)
print(data_new_5) # è½¬æ¢å®Œæˆ
```

1. æ ‡å‡†åŒ– StandardScaler

```python
# x' = (x - mean)/sigma
# mean ä¸ºå¹³å‡å€¼, sigma ä¸ºæ ‡å‡†å·®

# 1.è·å–æ•°æ®
# 2.å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
pre_transfer = StandardScaler()

# 3.è°ƒç”¨fit_transform
data_new_5 = pre_transfer.fit_transform(data_5)
print(data_new_5)  # è½¬æ¢å®Œæˆ
```

### ç‰¹å¾é™ç»´

> å¯¹è±¡ï¼šäºŒç»´æ•°ç»„ â†’ é™ä½ç‰¹å¾ä¸ªæ•°ï¼Œå¾—åˆ°ä¸€ç»„â€œä¸ç›¸å…³â€ä¸»å˜é‡çš„è¿‡ç¨‹ï¼Œä»¥è¾¾åˆ°ç‰¹å¾ä¸ç‰¹å¾ä¹‹é—´ä¸ç›¸å…³

1. ç‰¹å¾é€‰æ‹©

- æ•°æ®ä¸­åŒ…å«å†—ä½™æˆ–ç›¸å…³å˜é‡(ç‰¹å¾ï¼Œå±æ€§ï¼ŒæŒ‡æ ‡)ï¼Œæ—¨åœ¨ä»åŸæœ‰ç‰¹å¾ä¸­æ‰¾å‡ºä¸»è¦ç‰¹å¾
- 1.Filter(è¿‡æ»¤å¼)ï¼šæ¢ç©¶ç‰¹å¾æœ¬èº«ç‰¹ç‚¹ï¼Œç‰¹å¾ä¸ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´å…³è” - æ–¹å·®é€‰æ‹©æ³•ï¼Œç›¸å…³ç³»æ•°æ³•
- 2.Embedded(åµŒå…¥å¼)ï¼šç®—æ³•è‡ªåŠ¨é€‰æ‹©ç‰¹å¾(ç‰¹å¾ä¸ç›®æ ‡å€¼ä¹‹é—´çš„å…³è”) - å†³ç­–æ ‘ï¼Œæ­£åˆ™åŒ–ï¼Œå·ç§¯

API

```python
sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
# åˆ é™¤æ‰€æœ‰ä½æ–¹å·®ç‰¹å¾
# Variance.fit_transform(X) X :numpy arrayæ ¼å¼çš„æ•°æ®, é»˜è®¤å€¼æ˜¯ä¿ç•™æ‰€æœ‰éé›¶æ–¹å·®ç‰¹å¾
```

- threshold : ä¸´ç•Œå€¼

```python
# # 1.è·å–æ•°æ®
data_6 = pd.read_csv("D:/dev/resources/MLR/æœºå™¨å­¦xiday1èµ„æ–™/02-ä»£ç /factor_returns.csv")
data_6 = data_6.iloc[:, 1:10]

# # 2.å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
fs_transfer = VarianceThreshold()

# # 3.è°ƒç”¨fit_transform
data_new_6 = fs_transfer.fit_transform(data_6)
print(data_new_6)
```

1. ç›¸å…³ç³»æ•° r 0.4 | 0.7 | 1

```python
from scipy.stats import pearsonr

# è®¡ç®—æŸä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°
r = pearsonr(data_6["pe_ratio"], data_6["pb_ratio"])
print(r)

# ç‰¹å¾ä¸ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§å¾ˆé«˜ : 1.é€‰å…¶ä¸­ä¸€ä¸ª 2.åŠ æƒæ±‚å’Œ 3.ä¸»æˆåˆ†åˆ†æ ->
```

1. ä¸»æˆåˆ†åˆ†æ(PCA)

> å®šä¹‰ï¼šé«˜ç»´æ•°æ®è½¬æ¢ä¸ºåœ°ä½æ•°æ®çš„è¿‡ç¨‹ï¼Œæ­¤è¿‡ç¨‹å¯èƒ½ä¼šèˆå»åŸæœ‰æ•°æ®ï¼Œåˆ›é€ æ–°çš„å˜é‡                         ä½œç”¨ï¼šæ•°æ®ç»´æ•°å‹ç¼©ï¼Œå°½å¯èƒ½é™ä½åŸæ•°æ®çš„ç»´æ•°ï¼ŒæŸå¤±å°‘é‡ä¿¡æ¯                                                                        åº”ç”¨ï¼šå›å½’åˆ†ææˆ–èšç±»åˆ†æä¸­

```python
from sklearn.decomposition import PCA

# API : sklearn.decomposition.PCA(n_components=None) n_components : å°æ•° è¡¨ç¤ºä¿ç•™ç™¾åˆ†ä¹‹å¤šå°‘çš„ä¿¡æ¯, æ•´æ•° å‡å°‘åˆ°å¤šå°‘ç‰¹å¾
# PCA.fit_transform(X) X : numppy array æ ¼å¼çš„æ•°æ®, è¿”å›æŒ‡å®šç»´åº¦çš„æ•°ç»„
data_7 = [[2, 8, 4, 5],
          [6, 3, 0, 8],
          [5, 4, 9, 1]]

# # 1.å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
pca_transfer = PCA(n_components=2)  # è¡¨ç¤ºè½¬æ¢ä¸ºä¸¤ä¸ªç‰¹å¾

# 2.è°ƒç”¨fit_transform
data_new_7 = pca_transfer.fit_transform(data_7)
print(data_new_7)
```

## å®æˆ˜

```python
# æ¢ç©¶Instacartæ¶ˆè´¹è€…å°†è´­ä¹°å“ªäº›äº§å“

# 1.è·å–æ•°æ®
order_products = pd.read_csv("D:/dev/resources/MLR/æœºå™¨å­¦xiday1èµ„æ–™/02-ä»£ç /instacart/order_products__prior.csv")
products = pd.read_csv("D:/dev/resources/MLR/æœºå™¨å­¦xiday1èµ„æ–™/02-ä»£ç /instacart/products.csv")
orders = pd.read_csv("D:/dev/resources/MLR/æœºå™¨å­¦xiday1èµ„æ–™/02-ä»£ç /instacart/orders.csv")
aisle = pd.read_csv("D:/dev/resources/MLR/æœºå™¨å­¦xiday1èµ„æ–™/02-ä»£ç /instacart/aisles.csv")

# 2.åˆå¹¶è¡¨
tab1 = pd.merge(aisle, products, on=["aisle_id", "aisle_id"])
tab2 = pd.merge(tab1, order_products, on=["product_id", "product_id"])
tab3 = pd.merge(tab2, orders, on=["order_id", "order_id"])

# 3.æ‰¾åˆ°user_idå’Œaisleä¹‹é—´çš„å…³ç³»
data = pd.crosstab(tab3["user_id"], tab3["aisle"])[:10000]

# 4.PCAé™ç»´
pca_transfer = PCA(n_components=0.95)
data_new = pca_transfer.fit_transform(data)

print(data_new)
```

----

# T

## è½¬æ¢å™¨å’Œé¢„ä¼°å™¨

- è½¬æ¢å™¨ transformerï¼šç‰¹å¾å·¥ç¨‹ - 1.å®ä¾‹åŒ– 2.è°ƒç”¨fit_transform

- é¢„ä¼°å™¨ estimatorï¼šæ˜¯ä¸€ç±»å®ç°äº†ç®—æ³•çš„APIï¼Œåˆ†ç±»ï¼Œå›å½’ï¼Œæ— ç›‘ç£å­¦ä¹ ç­‰ç®—æ³•éƒ½æ˜¯å…¶å­ç±»           å·¥ä½œæµç¨‹ï¼šå®ä¾‹åŒ–estimator â†’ estimator.fit(x_test, y_test) æµ‹è¯•é›†è®¡ç®—ï¼Œè®­ç»ƒ â†’ è°ƒç”¨å®Œæ¯•ï¼Œæ¨¡å‹ç”Ÿæˆ â†’ æ¨¡å‹è¯„ä¼° 1. å¯¹æ¯”çœŸå®å€¼å’Œé¢„æµ‹å€¼ y_predict=estimator.predict(x_test), y_test==y_predict?

  ```
                                2.è®¡ç®—å‡†ç¡®ç‡ estimator.score(x_test, y_test)
  ```

## K-è¿‘é‚»ç®—æ³• KNN

- æ ¸å¿ƒæ€æƒ³ï¼šæ ¹æ®é‚»å±…åˆ¤æ–­ç±»åˆ«
- è·ç¦»å…¬å¼ï¼šæ¬§å¼è·ç¦»ï¼Œæ›¼å“ˆé¡¿è·ç¦»ï¼Œæ˜å¯å¤«æ–¯åŸºè·ç¦»
- æ— é‡çº²åŒ–å¤„ç†ï¼šæ ‡å‡†åŒ–

API

```python
sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto')
```

å‚æ•°è¯´æ˜ï¼š

- n_neighborsï¼šç”¨äºåˆ†ç±»çš„é‚»å±…æ•°é‡ï¼Œé»˜è®¤ä¸º5
- algorithmï¼šç”¨äºè®¡ç®—æœ€è¿‘é‚»çš„ç®—æ³•ï¼Œå¯é€‰'auto'ã€'ball_tree'ã€'kd_tree'æˆ–'brute'

KNNç®—æ³•çš„ä¼˜ç¼ºç‚¹ï¼š ä¼˜ç‚¹ï¼šç®€å•æ˜“æ‡‚ï¼Œæ— éœ€è®­ç»ƒï¼Œå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ ç¼ºç‚¹ï¼šæ‡’æƒ°ç®—æ³•ï¼Œè®¡ç®—é‡å¤§ï¼Œå†…å­˜æ¶ˆè€—å¤§ï¼Œé¢„æµ‹æ•ˆç‡ä½ï¼Œå¯¹ç‰¹å¾æ•°é‡æ•æ„Ÿï¼›å¿…é¡»æŒ‡å®škå€¼ï¼Œkå€¼é€‰æ‹©ä¸å½“åˆ™åˆ†ç±»ç²¾åº¦ä¸èƒ½ä¿è¯

ä½¿ç”¨åœºæ™¯ï¼šå°è§„æ¨¡æ•°æ®ï¼ŒèŒƒå›´ 1000 ~ 99999

æ¡ˆä¾‹ ï¼širisåˆ†ç±»

```python
# 1.è·å–æ•°æ®
    iris = load_iris()

    # 2.åˆ’åˆ†æ•°æ®é›†
    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=6)

    # 3.ç‰¹å¾å·¥ç¨‹ : æ ‡å‡†åŒ–
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)  # ç”¨transformæ˜¯å› ä¸ºä¸Šä¸€æ­¥å·²ç»å¯¹x_train fit è¿‡äº†(æ±‚è¿‡äº†æ ‡å‡†å·®ç­‰)

    # 4.KNNç®—æ³•é¢„ä¼°å™¨
    estimator = KNeighborsClassifier(n_neighbors=3)
    estimator.fit(x_train, y_train)

    # 5.æ¨¡å‹è¯„ä¼°
    # 1)å¯¹æ¯”çœŸå®å€¼å’Œé¢„æµ‹å€¼
    y_predict = estimator.predict(x_test)
    print(y_test == y_predict)

    # 2)è®¡ç®—å‡†ç¡®ç‡
    score = estimator.score(x_test, y_test)
    print(score)
```

## æ¨¡å‹é€‰æ‹©ä¸è°ƒä¼˜

1. cross validation äº¤å‰éªŒè¯ï¼šå°†æ‹¿åˆ°çš„è®­ç»ƒæ•°æ®ï¼Œåˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†

   å¦‚ï¼šå°†æ•°æ®åˆ†ä¸ºå››ä»½ï¼Œæ¯ä»½å–ä¸€ä»½ä¸ºéªŒè¯é›†ï¼Œç»è¿‡å››ç»„æµ‹è¯•ï¼Œæ¯æ¬¡æ›´æ¢éªŒè¯é›†ï¼Œå°†å››ç»„æ¨¡å‹ç»“æœå–å¹³å‡å€¼ï¼Œåˆç§°4æŠ˜äº¤å‰éªŒè¯

2. Grid Search è¶…å‚æ•°æœç´¢-ç½‘æ ¼æœç´¢ï¼šæœ‰å¾ˆå¤šå‚æ•°æ˜¯è¦æ‰‹åŠ¨æŒ‡å®šçš„(å¦‚KNNä¸­çš„kå€¼)ï¼Œè¿™ç§å«è¶…å‚æ•°ï¼Œæ¯ç»„è¶…å‚æ•°éƒ½é‡‡ç”¨äº¤å‰éªŒè¯æ¥è¯„ä¼°ï¼Œæœ€åé€‰å‡ºæœ€ä¼˜å‚æ•°å»ºç«‹æ¨¡å‹

API

```python
sklearn.model_selection.GridSearchCV(estimator, param_grid=None, cv=None)
```

- estimatorï¼šä¼°è®¡å™¨
- param_gridï¼šä¼°è®¡å™¨å‚æ•°(dict){â€n_neighborsâ€:[1, 3, 5]}
- cvï¼šæŒ‡å®šå‡ æŠ˜äº¤å‰éªŒè¯
- fit()ï¼šè¾“å…¥è®­ç»ƒæ•°æ®
- score()ï¼šå‡†ç¡®ç‡
- ç»“æœåˆ†æï¼š
  - æœ€ä½³å‚æ•°ï¼šbest_params_
  - æœ€ä½³ç»“æœï¼šbest_score_
  - æœ€ä½³ä¼°è®¡å™¨ï¼šbest_estimator_
  - äº¤å‰éªŒè¯ç»“æœï¼šcv_results_

æ¡ˆä¾‹ï¼širisåˆ†ç±»ï¼Œå¹¶æ·»åŠ ç½‘æ ¼æœç´¢å’Œäº¤å‰éªŒè¯

```python
# 1.è·å–æ•°æ®
    iris = load_iris()

    # 2.åˆ’åˆ†æ•°æ®é›†
    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=6)

    # 3.ç‰¹å¾å·¥ç¨‹ : æ ‡å‡†åŒ–
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)  # ç”¨transformæ˜¯å› ä¸ºä¸Šä¸€æ­¥å·²ç»å¯¹x_train fit è¿‡äº†(æ±‚è¿‡äº†æ ‡å‡†å·®ç­‰)

    # 4.KNNç®—æ³•é¢„ä¼°å™¨
    estimator = KNeighborsClassifier()

    # 5.åŠ å…¥GridSearchCV
    # å‚æ•°å‡†å¤‡
    param_dict = {"n_neighbors": [1, 3, 5, 7, 9, 11]}  # æƒ³è¦æµ‹è¯•çš„kå€¼
    estimator = GridSearchCV(estimator, param_grid=param_dict, cv=10)

    estimator.fit(x_train, y_train)

    # 6.æ¨¡å‹è¯„ä¼°
    # 1)å¯¹æ¯”çœŸå®å€¼å’Œé¢„æµ‹å€¼
    y_predict = estimator.predict(x_test)
    print(y_test == y_predict)

    # 2)è®¡ç®—å‡†ç¡®ç‡
    score = estimator.score(x_test, y_test)
    print(score)

    print("best_params_:", estimator.best_params_)
    print("best_score_:", estimator.best_score_)
    print("best_estimator_:", estimator.best_score_)
    print("cv_results_:", estimator.cv_results_)
```

## æœ´ç´ è´å¶æ–¯ç®—æ³• Native Bayes

- Bayesâ€™ theorem è´å¶æ–¯å…¬å¼ï¼š P(B|A) = P(A) / P(B)ï¼Œåœ¨Bå·²ç»å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼ŒAå‘ç”Ÿçš„æ¦‚ç‡
- Native Bayes æœ´ç´ è´å¶æ–¯ï¼šå‡è®¾ç‰¹å¾ä¸ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ P(A, B) = P(A)P(B)
- Laplacian smoothing æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°ï¼šP(F1|C) = (Ni + a) / (N + am), ç›®çš„æ˜¯é˜²æ­¢è®¡ç®—å‡ºçš„åˆ†ç±»æ¦‚ç‡ä¸ºé›¶ï¼ŒaæŒ‡å®šç³»æ•°ä¸€èˆ¬ä¸º1ï¼Œmä¸ºè®­ç»ƒæ–‡æ¡£ä¸­ç»Ÿè®¡å‡ºç‰¹å¾è¯çš„ä¸ªæ•°

API

```python
sklearn.naive_bayes.MultinomialINB(alpha=1.0)
```

- alpha = 1.0ï¼šè¡¨ç¤ºä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼ˆLaplace smoothingï¼‰ï¼Œè¿™æ˜¯æœ€å¸¸ç”¨çš„è®¾ç½®
- alpha < 1.0ï¼šä¼šå¢å¼ºå¹³æ»‘æ•ˆæœï¼Œé€‚ç”¨äºæ ·æœ¬é‡å°çš„æƒ…å†µ
- alpha > 1.0ï¼šå‡å¼±å¹³æ»‘æ•ˆæœï¼Œé€‚ç”¨äºæ ·æœ¬é‡å¤§çš„æƒ…å†µ

å®ä¾‹

```python
  	# 1.è·å–æ•°æ®
    news = fetch_20newsgroups(subset="all")

    # 2.åˆ’åˆ†æ•°æ®é›†
    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target)

    # 3.ç‰¹å¾å·¥ç¨‹ : æ–‡æœ¬ç‰¹å¾æŠ½å–-tfidf
    transfer = TfidfVectorizer()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)

    # 4.æœ´ç´ è´å¶æ–¯ç®—æ³•é¢„ä¼°å™¨æµç¨‹
    estimator = MultinomialNB()
    estimator.fit(x_train, y_train)

    # 5.æ¨¡å‹è¯„ä¼°
    # 1)å¯¹æ¯”çœŸå®å€¼å’Œé¢„æµ‹å€¼
    y_predict = estimator.predict(x_test)
    print(y_test == y_predict)

    # 2)è®¡ç®—å‡†ç¡®ç‡
    score = estimator.score(x_test, y_test)
    print(score)
```

## å†³ç­–æ ‘ Decision_tree

> ä¿¡æ¯è®ºåŸºç¡€ : ä¿¡æ¯çš„è¡¡é‡ - ä¿¡æ¯é‡ - ä¿¡æ¯ç†µ H(x) = -âˆ‘P(xi)logb(P(xi))

1. å†³ç­–æ ‘åˆ†ç±»å™¨

```python
sklearn.tree.DecisionTreeClassifier(criterion="gini", max_depth=None, random_state=None)
```

- criterion : é»˜è®¤giniç³»æ•°, ä¹Ÿå¯ä»¥é€‰æ‹©ä¿¡æ¯å¢ç›Šçš„ç†µentropy
- max_depth : æ ‘çš„æ·±åº¦
- random_state : éšæœºç§å­

1. å†³ç­–æ ‘å¯è§†åŒ–

```python
sklearn.tree.export_graphviz(estimator, out_file=" ", feature_names=[" ", " "]  
```

- è¯¥æ–‡ä»¶å¯ä»¥å¯¼å‡ºä¸ºDOTæ ¼å¼
- out_file : æ–‡ä»¶è·¯å¾„
- feature_names : ç‰¹å¾åç§°
- ä¼˜ç‚¹ : ç®€å•çš„ç†è§£å’Œè§£é‡Š, å¯è§†åŒ– ç¼ºç‚¹ : å¯èƒ½ä¼šè¿‡æ‹Ÿåˆ

å®ä¾‹

```python
# 1.è·å–æ•°æ®é›†
    iris = load_iris()

    # 2.åˆ’åˆ†æ•°æ®é›†
    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)

    # 3.å†³ç­–æ ‘é¢„ä¼°å™¨
    estimator = DecisionTreeClassifier(criterion="entropy")
    estimator.fit(x_train, y_train)

    # 4.æ¨¡å‹è¯„ä¼°
    # 1)å¯¹æ¯”çœŸå®å€¼å’Œé¢„æµ‹å€¼
    y_predict = estimator.predict(x_test)
    print(y_test == y_predict)

    # 2)è®¡ç®—å‡†ç¡®ç‡
    score = estimator.score(x_test, y_test)
    print(score)

    # 5.å†³ç­–æ ‘å¯è§†åŒ–
    # export_graphviz(estimator, out_file="iris_tree.dot")  åœ¨webgraphviz.comä¸­æŸ¥çœ‹æ ‘
```

## éšæœºæ£®æ— Random_forest

- ä¸¤ä¸ªéšæœº :
  - 1.è®­ç»ƒé›†éšæœº bootstrap éšæœºæ”¾å›æŠ½æ ·
  - 2.ç‰¹å¾éšæœº ä»Mä¸ªç‰¹å¾ä¸­éšæœºæŠ½å–mä¸ªç‰¹å¾ M >> m, èµ·åˆ°é™ç»´çš„æ•ˆæœ

API

```python
sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion="gini", max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)
```

- éšæœºæ£®æ—åˆ†ç±»å™¨
  - n_estimators : æ£®æ—ä¸­çš„æ ‘æœ¨æ•°é‡
  - criterion : åˆ†å‰²ç‰¹å¾çš„æµ‹é‡æ–¹æ³•
  - max_depth : æ ‘çš„æœ€å¤§æ·±åº¦
  - bootstrap : æ˜¯å¦åœ¨æ„å»ºæ ‘æ—¶é‡‡ç”¨æ”¾å›æŠ½æ ·
  - min_samples_split : èŠ‚ç‚¹åˆ’åˆ†çš„æœ€å°æ ·æœ¬æ•°
  - min_samples_leaf : å¶å­èŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ•°

----

# F

## çº¿æ€§å›å½’ Linear_regression

$$ y = w{_1}x{_1}+w{_2}x{_2}+...+w{_n}x{_n}+b $$

- bä¸ºåç½®

<aside> ğŸ’¡

çº¿æ€§å…³ç³»ä¸€å®šæ˜¯çº¿æ€§æ¨¡å‹

çº¿æ€§æ¨¡å‹ä¸ä¸€å®šæ˜¯çº¿æ€§å…³ç³»

</aside>

ä¼˜åŒ–ä½¿æŸå¤±å‡å°ï¼Œæ¥è¿‘çœŸå®å€¼ï¼š

1. æ­£è§„æ–¹ç¨‹

   ç›´æ¥è®¡ç®—w

   API

   ```python
   sklearn.linear_model.LinearRegression(fit_intercept=True)
   ```

   - fit_interceptï¼šæ˜¯å¦è®¡ç®—åç½®
   - LinearRegression.coef_ï¼šå›å½’ç³»æ•°
   - LinearRegression.intercept_ï¼šåç½®

2. æ¢¯åº¦ä¸‹é™

   > å®ç°äº†éšæœºæ¢¯åº¦ä¸‹é™å­¦ä¹ ï¼Œæ”¯æŒä¸åŒçš„çš„loså‡½æ•°å’Œæ­£åˆ™åŒ–æƒ©ç½šé¡¹æ¥æ‹Ÿåˆçº¿æ€§å›å½’æ¨¡å‹

   ä¸æ–­è¿­ä»£w

   API

   ```python
   sklearn.linear_model.SGDRegression(loss="squared_loss", fit_intercept=True, 
   learning_rate="invscaling", eta0=0.01)
   ```

   - lossï¼šæŸå¤±ç±»å‹ï¼Œâ€squared_lossâ€æ™®é€šæœ€å°äºŒä¹˜æ³•

   - fit_interceptï¼šæ˜¯å¦è®¡ç®—åç½®

   - learning_rateï¼šå­¦ä¹ ç‡å¡«å……â€constantâ€ â€œoptimalâ€ invscalingâ€

     ```
     å¯¹ä¸€ä¸ªå¸¸æ•°å€¼çš„å­¦ä¹ ç‡æ¥è¯´ï¼Œå¯ä»¥ä½¿ç”¨learning_rate=â€constantâ€ï¼Œå¹¶é€šè¿‡eta0æ¥æŒ‡å®šå­¦ä¹ ç‡
     ```

   - SGDRegression.coef_ï¼šå›å½’ç³»æ•°

   - SGDRegression.intercept_ï¼šåç½®

3. å›å½’æ€§èƒ½è¯„ä¼° Mean Squared Error è¯„ä»·æœºåˆ¶

   $$ MSE =1/m{\ }*{\ }Î£(y{_i}-y{_a}{_b})^2 $$

API

```python
sklearn.metrics.mean_squared_error(y_true, y_pred)
```

- y_trueï¼šçœŸå®å€¼
- y_predï¼šæµ‹è¯•å€¼
- returnï¼šæµ®ç‚¹æ•°

æ¡ˆä¾‹ï¼šæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹

```python
# è·å–æ•°æ®
    data_url = "<http://lib.stat.cmu.edu/datasets/boston>"
    raw_df = pd.read_csv(data_url, sep="\\s+", skiprows=22, header=None)
    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    target = raw_df.values[1::2, 2]

    # åˆ’åˆ†æ•°æ®é›†
    x_train, x_test, y_train, y_test = train_test_split(data, target, random_state=22)

    # ç‰¹å¾å·¥ç¨‹
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)

    # é¢„ä¼°å™¨
    estimator = LinearRegression()  # æ­£è§„æ–¹ç¨‹
    # estimator = SGDRegressor()  # æ¢¯åº¦ä¸‹é™
    estimator.fit(x_train, y_train)

    print("coef: ", estimator.coef_)
    print("intercept: ", estimator.intercept_)

    # æ¨¡å‹è¯„ä¼° (ä¸åŒäºä¹‹å‰)
    y_predict = estimator.predict(x_test)
    print("æˆ¿ä»·é¢„æµ‹: ", y_predict)
    error = mean_squared_error(y_test, y_predict)
    print("å‡æ–¹è¯¯å·®: ", error)
```

## å²­å›å½’ Ridge

> æ¬ æ‹Ÿåˆ underfittingï¼šä¸€ä¸ªå‡è®¾åœ¨è®­ç»ƒæ•°æ®ä¸Šä¸èƒ½è·å¾—æ›´å¥½çš„æ‹Ÿåˆï¼Œå¹¶ä¸”åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šä¹Ÿä¸èƒ½å¾ˆå¥½çš„æ‹Ÿåˆæ•°æ®ï¼Œæ­¤æ—¶è®¤ä¸ºè¿™ä¸ªå‡è®¾å‡ºç°äº†æ¬ æ‹Ÿåˆçš„ç°è±¡ã€‚è§£å†³æ–¹æ³•ï¼šå¢åŠ æ•°æ®ç‰¹å¾æ•°é‡            è¿‡æ‹Ÿåˆ overfittingï¼šä¸€ä¸ªå‡è®¾åœ¨è®­ç»ƒæ•°æ®é›†èƒ½å¤Ÿè·å¾—æ¯”å…¶ä»–å‡è®¾æ›´å¥½çš„æ‹Ÿåˆï¼Œä½†æ˜¯åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå´ä¸èƒ½å¾ˆå¥½çš„æ‹Ÿåˆæ•°æ®ï¼Œè¿™å°±æ˜¯è¿‡æ‹Ÿåˆç°è±¡ã€‚è§£å†³æ–¹æ³•ï¼šæ­£åˆ™åŒ–

API

```python
sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, solver="auto", 
normalizer=False)
```

- alphaï¼šæ­£åˆ™åŒ–åŠ›åº¦[0, 1] | [1, 10]
- solverï¼šæ ¹æ®æ•°æ®è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚æœæ•°æ®é›†ï¼Œç‰¹å¾éƒ½æ¯”è¾ƒå¤§ï¼Œç”¨SAG
- normalizeï¼šæ•°æ®æ˜¯å¦è¿›è¡Œæ ‡å‡†åŒ–
- estimator.coef_ï¼šå›å½’æƒé‡
- estimator.intercept_ï¼šå›å½’åç½®

æ¡ˆä¾‹ï¼šæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹

```python
# è·å–æ•°æ®
    data_url = "<http://lib.stat.cmu.edu/datasets/boston>"
    raw_df = pd.read_csv(data_url, sep="\\s+", skiprows=22, header=None)
    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    target = raw_df.values[1::2, 2]

    # åˆ’åˆ†æ•°æ®é›†
    x_train, x_test, y_train, y_test = train_test_split(data, target, random_state=22)

    # ç‰¹å¾å·¥ç¨‹
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)

    # é¢„ä¼°å™¨
    estimator = Ridge()  # æ­£è§„æ–¹ç¨‹
    estimator.fit(x_train, y_train)

    print("coef: ", estimator.coef_)
    print("intercept: ", estimator.intercept_)

    # æ¨¡å‹è¯„ä¼° (ä¸åŒäºä¹‹å‰)
    y_predict = estimator.predict(x_test)
    print("æˆ¿ä»·é¢„æµ‹: ", y_predict)
    error = mean_squared_error(y_test, y_predict)
    print("å‡æ–¹è¯¯å·®: ", error)
```

## é€»è¾‘å›å½’  Logistic_regression

> æ˜¯ä¸€ç±»åˆ†ç±»æ¨¡å‹ï¼Œé€»è¾‘å›å½’çš„ä¸€ä¸ªè¾“å…¥å°±æ˜¯çº¿æ€§å›å½’çš„è¾“å‡º

API

```python
sklearn.linear_model.LogisticRegression(solver="liblinear", penalty="l2", C=1.0)
```

- solverï¼šä¼˜åŒ–æ±‚è§£æ–¹æ³•
- penaltyï¼šæ­£åˆ™åŒ–ç§ç±»
- Cï¼šæ­£åˆ™åŒ–åŠ›åº¦

åˆ†ç±»çš„è¯„ä¼°æ–¹æ³•ï¼šç²¾å‡†ç‡å’Œå¬å›ç‡(Precisoin&Recall)

API

```python
sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None)
```

- y_trueï¼šçœŸå®å€¼
- y_predï¼šä¼°è®¡å™¨é¢„ä¼°ç›®æ ‡å€¼
- labelsï¼šæŒ‡å®šç±»åˆ«å¯¹åº”çš„æ•°å­—
- target_namesï¼šç›®æ ‡ç±»åˆ«åç§°
- returnï¼šæ¯ä¸ªç±»åˆ«çš„ç²¾å‡†ç‡å’Œå¬å›ç‡

<aside> ğŸ’¡

æ ·æœ¬ä¸å‡è¡¡æ—¶å€ŸåŠ©ROCæ›²çº¿å’ŒAUCæŒ‡æ ‡ã€‚AUC[0.5, 1]ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½

</aside>

API

```python
sklearn.metrics.roc_auc_score(y_true, y_score)
```

- y_trueï¼šæ¯ä¸ªæ ·æœ¬çœŸå®ç±»åˆ«
- y_scoreï¼šé¢„æµ‹å¾—åˆ†

æ¡ˆä¾‹ï¼šç™Œç—‡åˆ†ç±»

```python
# è¯»å–æ•°æ®
    path = "<https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data>"
    column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',
                   'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',
                   'Normal Nucleoli', 'Mitoses', 'Class']
    data = pd.read_csv(path, names=column_name)

    # ç¼ºå¤±å€¼å¤„ç†
    data = data.replace(to_replace="?", value=np.nan)  # æ›¿æ¢ä¸ºNan
    data = data.dropna()  # åˆ é™¤æ ·æœ¬

    # ç­›é€‰ç‰¹å¾å€¼å’Œç›®æ ‡å€¼
    x = data.iloc[:, 1:-1]
    y = data["Class"]

    # åˆ’åˆ†æ•°æ®é›†
    x_train, x_test, y_train, y_test = train_test_split(x, y)

    # ç‰¹å¾å·¥ç¨‹ æ ‡å‡†åŒ–
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)

    # é€»è¾‘å›å½’
    estimator = LogisticRegression()
    estimator.fit(x_train, y_train)

    print("coef: ", estimator.coef_)
    print("intercept: ", estimator.intercept_)

    # æ¨¡å‹è¯„ä¼°
    # 1)å¯¹æ¯”çœŸå®å€¼å’Œé¢„æµ‹å€¼
    y_predict = estimator.predict(x_test)
    print(y_test == y_predict)

    # 2)è®¡ç®—å‡†ç¡®ç‡
    score = estimator.score(x_test, y_test)
    print("score: ", score)

    # æŸ¥çœ‹ç²¾ç¡®ç‡, å¬å›ç‡, F1-score
    report = classification_report(y_test, y_predict, labels=[2, 4], target_names=["è‰¯æ€§", "æ¶æ€§"])
    print(report)

    # æŸ¥çœ‹AUCæŒ‡æ ‡
    y_test = np.where(y_test > 2.5, 1, 0)

    print("AUC ", roc_auc_score(y_test, y_predict))
```

## KMeansç®—æ³•

K-Meansèšç±»æ­¥éª¤

- å¯¹äºå…¶ä»–æ¯ä¸ªç‚¹è®¡ç®—åˆ°Kä¸ªä¸­å¿ƒçš„è·ç¦»ï¼ŒæœªçŸ¥çš„ç‚¹é€‰æ‹©æœ€è¿‘çš„ä¸€ä¸ªèšç±»ä¸­å¿ƒç‚¹ä½œä¸ºæ ‡è®°ç±»åˆ«
- æ¥ç€å¯¹ç€æ ‡è®°çš„èšç±»ä¸­å¿ƒä¹‹åï¼Œé‡æ–°è®¡ç®—å‡ºæ¯ä¸ªèšç±»çš„æ–°ä¸­å¿ƒç‚¹ï¼ˆå¹³å‡å€¼ï¼‰
- å¦‚æœè®¡ç®—å¾—å‡ºçš„æ–°ä¸­å¿ƒç‚¹ä¸åŸä¸­å¿ƒç‚¹ä¸€æ ·ï¼Œé‚£ä¹ˆç»“æŸï¼Œå¦åˆ™é‡æ–°è¿›è¡Œç¬¬äºŒæ­¥è¿‡ç¨‹
- K-è¶…å‚æ•°: çœ‹éœ€æ±‚å–å€¼

API

```python
sklearn.clusters.KMeans(n_clusters=n, init="k-means++")
```

- n_clustersï¼šå¼€å§‹çš„èšç±»ä¸­å¿ƒé‡
- initï¼šåˆå§‹åŒ–æ–¹æ³•
- labelsï¼šé»˜è®¤æ ‡è®°ç±»å‹

<aside> ğŸ’¡

KMeansæ€§èƒ½è¯„ä¼°æŒ‡æ ‡ï¼š

"é«˜å†…èš, ä½è€¦åˆ" -> bi å¤–éƒ¨è·ç¦»æœ€å¤§åŒ–, ai å†…éƒ¨è·ç¦»æœ€å°åŒ–

è½®å»“ç³»æ•° SCi = (bi-ai) / max(bi, ai) [-1, 1], 1æ•ˆæœæœ€å¥½

</aside>

API

```python
sklearn.metrics.silhouette_score(X, labels)
```

- Xï¼šç‰¹å¾å€¼
- labelsï¼šè¢«èšç±»æ ‡è®°çš„ç›®æ ‡å€¼

ä¼˜ç‚¹ï¼šé‡‡ç”¨è¿­ä»£å¼ç®—æ³•ï¼Œç›´è§‚æ˜“æ‡‚ä¸”éå¸¸å®ç”¨

ç¼ºç‚¹ï¼šå®¹æ˜“æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£(å¤šæ¬¡èšç±»)

<aside> ğŸ’¡

èšç±»ä¸€èˆ¬åœ¨åˆ†ç±»å‰

</aside>

æ¡ˆä¾‹ï¼šK-Meanså¯¹Instacart Marketç”¨æˆ·è¿›è¡Œèšç±»

```python
  	# æ•°æ®é™ç»´
    # 1.è·å–æ•°æ®
    order_products = pd.read_csv("D:/dev/resources/MLR/æœºå™¨å­¦xiday1èµ„æ–™/02-ä»£ç /instacart/order_products__prior.csv")
    products = pd.read_csv("D:/dev/resources/MLR/æœºå™¨å­¦xiday1èµ„æ–™/02-ä»£ç /instacart/products.csv")
    orders = pd.read_csv("D:/dev/resources/MLR/æœºå™¨å­¦xiday1èµ„æ–™/02-ä»£ç /instacart/orders.csv")
    aisle = pd.read_csv("D:/dev/resources/MLR/æœºå™¨å­¦xiday1èµ„æ–™/02-ä»£ç /instacart/aisles.csv")

    # 2.åˆå¹¶è¡¨
    tab1 = pd.merge(aisle, products, on=["aisle_id", "aisle_id"])
    tab2 = pd.merge(tab1, order_products, on=["product_id", "product_id"])
    tab3 = pd.merge(tab2, orders, on=["order_id", "order_id"])

    # 3.æ‰¾åˆ°user_idå’Œaisleä¹‹é—´çš„å…³ç³»
    data = pd.crosstab(tab3["user_id"], tab3["aisle"])[:10000]

    # 4.PCAé™ç»´
    pca_transfer = PCA(n_components=0.95)
    data_new = pca_transfer.fit_transform(data)

    # é¢„ä¼°å™¨æµç¨‹
    estimator = KMeans()
    estimator.fit(data_new)

    y_predict = estimator.predict(data_new)

    # KMeansæ€§èƒ½æŒ‡æ ‡åˆ†æç»“æœ
    print("SCi ", silhouette_score(data_new, y_predict))
```

## æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½

```python
    import joblib

    joblib.dump(rf, "Season1.pkl")
    estimator = joblib.load("Season1.pkl")
```

