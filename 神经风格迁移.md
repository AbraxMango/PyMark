# 神经风格迁移

> 通过提取一张图片的风格信息，将其风格转换至另一张图片上，使其图片特征不变，但是风格变化
> 

为了提取图片的内容以及风格，使用一个预训练的VGG-19：conv1~conv5提取风格信息，

conv4提取内容信息

- 部分函数

```python
# 使用卷积层，从内容图像与样式图像中提取特征图
import torchvision.models as models
from torchvision.models import VGG19_Weights
# 从torchvision.models中取得已经训练好的VGG-19模型
# 使用.features获取特征处理的卷积层部分，使用.eval选择评估模式
cnn = models.vgg19(weights = VGG19_Weights.DEFAULT).features.eval()
print(cnn) # 打印获得到的cnn模型

import torch.nn as nn
# 基于卷积层的位置，对这些层进行编号，并专门使用其中的某几个卷积层
cnt = 0 # 表示卷积层的位置
model = nn.Sequential() # 设置一个空的model
# 遍历cnn.children()，重新命名模型中的每个层
for layer in cnn.children():
		if isinstance(layer, nn.Conv2d): # 当遇到卷积层时
				cnt += 1 # 记录conv层的数量 
				name = 'conv_{}'.format(cnt)
		# 用这个数量，为其他层如Relu和Pool进行编号与命名
		elif isinstance(layer, nn.Relu):
				name = 'relu_{}'.format(cnt)
		elif isinstance(layer, nn.MaxPool2d):
				name = 'pool_{}'.format(cnt)
		else
				name = 'other_{}'.format(cnt)
		# 将每层的名字和该层的对象，添加至model中
		model.add_module(name, layer)
print(model) # 打印模型
```

```python
from PIL import Image
from torchvision import transforms
# 实现图片读取函数
def img_loader(img_name, target_size):
		img = Image.open(img_name)
		# 重新定义图片的尺寸，保证样式图像与内容的大小是一致的
		img = img.resize(target_size)
		transform = transforms.ToTensor()  # 将图片转换为张量
		img = transform(img).unsqueeze(0)
		return img
		
# 读取图像内容
content_size = Image.open("neirong.path").size
style_img = img_loader("fengge.path", target_size = content_size)
content_img = img_loader("neirong.path", target_size = content_size)
```

```python
import torch
# 为了更明显的观察神经网络提取信息的过程，实现一个获取特征图的函数
def get_feature_map(model, img):
		feature_map = []
		# 遍历models中的各个层
		for name, layer in model.named_children():
				img = layer(img)
				feature_map.append((name, img))
		outputs = list()
		# 计算每层的平均特征图，结果保存在output中
		for name, feature in feature_map:
				feature = feature.squeeze(0)
				gray_scale = torch.sum(feature, 0)
				gray_scale = gray_scale / feature.shape[0]
				outputs.append((name, gray_scale.data.numpy()))
		return outputs
```

```python
import os
import matplotlib.pyplot as plt
if not os.path.exists('feature_maps'):
		os.makedirs('features_maps')
		
# 计算style_img在卷积神经网络中的各层平均特征图
style_maps = get_feature_map(model, style_img)
style_layers = {'conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'}
for name, img in style_maps:
		if name in style_layers:
				fig = plt.imshow(img)
				plt.savefig('./feature_maps/' + 'style_' + name + '.jpg')
# 计算content_img在卷积神经网络中的各层平均特征图
content_maps = get_feature_map(model, style_img)
content_layers = {'conv_4'}
for name, img in content_maps:
		if name in content_layers:
				fig = plt.imshow(img)
				plt.savefig('./feature_maps/' + 'content_' + name + '.jpg')
```

使用均方误差计算图像间的特征差异

```python
# 计算新生成图像在内容方面的损失值
class ContentLoss(nn.Module):
		def __init__(self, target):
				# 将原图像的特征图作为target输入
				super(ContentLoss, self).__init__()
				self.target = target.detach()
		def forward(self, input):
				# 计算新图像input与target的均方误差 
				self.loss = F.mse_loss(input, self.target)
				return input
				
# 格拉姆矩阵用于度量特征图中的不同通道之间的相关性 
# 计算特征图target的格拉姆矩阵
def gram_matrix(target):
		b, c, h, w = target.size()  # 批量大小，通道数，高度，宽度
		# 重塑张量，将原四位的target，转为2维的矩阵
		# 矩阵的行是b*c, 列是h*w
		features = target.view(b*c, h*w)
		# 计算features乘features的转置，结果代表了不同特征图之间的相关性
		G = torch.mm.(features, features.t())
		return G.div(b * c * h * w)  # 使用div对G进行规范化
		
# 计算风格损失
class StyleLoss(nn.Module):
		def __init__(self, target):
				super(StyleLoss, self).__init__()
				self.target = gram_matrix(target).detach()
		def forward(self, input):
				G = gram_matrix(input)
				self.loss = F.mse_loss(G, self.target)
				return input
```

```python
# 标准化层
class Normalization(nn.Module):
		def __init(self, mean, std):
				super(Normalization, self).__init__()
				self.mean = mean.clone().detach()
				self.std = std.clone().detach()
		def forward(selfm img):
				return (img - self.mean) / self.std
				
# 函数传入VGG-19网络cnn和样式图片style_img和内容图片conten_img
def create_model_and_losses(cnn, style_img, content_img):
		normal_mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)
		normal_std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)
		# 构造一个标准化层normal
		# 标准化层可以加快收敛速度，并防止梯度消失和梯度爆炸等问题
		normal = Normalization(normal_mean, normal_std)
		model = nn.Sequential(normal)  # 将normal添加至新的模型中
		conten_layers = {'conv_4'}  # 定义内容层名称集合
		style_layers = {'conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'} # 定义样式层的名称集合
		content_losses = []  # 保存内容损失的计算方法
		style_losses = []  # 保存样式损失的计算方法
		
		cnt = 0 # 表示卷积层的位置
		# 遍历cnn.children()，重新命名模型中的每个层
		for layer in cnn.children():
				if isinstance(layer, nn.Conv2d): # 当遇到卷积层时
						cnt += 1 # 记录conv层的数量 
						name = 'conv_{}'.format(cnt)
				# 用这个数量，为其他层如Relu和Pool进行编号与命名
				elif isinstance(layer, nn.Relu):
						name = 'relu_{}'.format(cnt)
				elif isinstance(layer, nn.MaxPool2d):
						name = 'pool_{}'.format(cnt)
				else
						name = 'other_{}'.format(cnt)
				# 将每层的名字和该层的对象，添加至model中
				model.add_module(name, layer)

		# 如果正在遍历的层在content_layers中
		if name in content_layers:
				# 将内容图片content_img输入至网络，提前计算出这个位置的特征图target
				target = model(content_img).detach()
				content_loss = ContentLoss(target)
				# 将计算生成图片内容损失的方法ContentLoss添加到model
		# 后续即可直接与ContentLoss中原始内容特征图计算MSE均方误差
				model.add_module("content_loss_{}".format(cnt), content_loss)
				content_losses.append(content_loss)

		# style同理
		if name in style_layers:
				target = model(style_img).detach()
				content_loss = ContentLoss(target)
				model.add_module("style_loss_{}".format(cnt), style_loss)
				style_losses.append(style_loss)
		
		i = 0
		# 将无关的层截取掉，来提升计算的速度
		for i in range(len(model)) -1, -1, -1):
				if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
						break
		model = model[:(i + 1)]
		return model, style_loss, content_loss
```

- 主程序

```python
import torchvision.models as models
from torchvision.models import VGG19_Weights
import torch.optim as optim
import os

if __name__ == "__main__"
		content_size = Image.open("neirong.path").size  # 内容图像
		style_img = img_loader("fengge.path", target_size = content_size)
		content_img = img_loader("neirong.path", target_size = content_size)
		
		# 定义VGG-19网络cnn
		cnn = models.vgg19(weights=VGG19_Weights.DEFAULT).features.eval()
		# 创建迭代用的模型models和损失函数style_losses, 和content_losses
		model, style_losses, content_losses = create_model_and_losses(cnn, style_img, content_img)
		
		# 不需要迭代模型，所以将模型调整为评估模式，并设置梯度为False
		model.eval()
		model.requires_grad_(False)
		
		# 定义生成图片，初始时，直接复制内容图片中的信息
		generate_img = content_img.clone()
		# 将该图片中的像素点的梯度设置为True，用于迭代
		generate_img.requires_grad_(True)
		# 使用LBFGS优化器，在风格迁移问题上，LBFGS比Adam等其他优化器更有效
		optimizer = optim.LBFGS([generate_img])
		
		# 创建generate_pic文件夹，保存迭代结果
		save_path = './generate_pic/'
		if not os.path.exists(save_path):
				os.makedirs(save_path)
		# 将每轮的结果都保存下来
		epoches = 100
		
		epoch = [0]
		while epoch[0] < epoches:
				def closure():  # 在闭包closure中实现循环迭代
						with torch.no_grad():
								generate_img.clamp_(0, 1)
						optimizer.zero_grad()
						model(generate_img)  # 使用model计算当前生成图像的前向传播后
						style_loss = 0  # 风格损失
						content_loss = 0  # 内容损失
						for sl in style_losses:
								style_loss += sl.loss
						for cl in content_losses:
								content_loss += ct.loss
						# 将style_loss放大100万倍，增加风格特征的重要性
						style_loss = style_loss * 1000000
						loss = style_loss + content_loss  # 累加两种损失到loss中
						
						loss.backward()  # 调用backward计算梯度
						
						epoch[0] += 1
						# 打印调试信息
						print(f"Epoch {epoch[0]}: Style loss: {style_loss.item():4f}"
									f"Content loss: {content_loss.item():4f}")
						# 并保存迭代结果
						imgsave(generate_img, save_path + f"epoch{epoch[0]}.jpg")
						return loss
				optimizer.step(closure)  # 迭代生成图片
```