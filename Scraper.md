# Scraper

## Request

> requestæ˜¯ä¸€ä¸ªç®€å•çš„Python HTTPè¯·æ±‚åº“

- ä½œç”¨æ˜¯å‘é€è¯·æ±‚è·å–å“åº”æ•°æ®

ä½¿ç”¨ï¼š

1. å¯¼å…¥æ¨¡å—
2. å‘é€getè¯·æ±‚ï¼Œè·å–å“åº”
3. ä»å“åº”ä¸­è·å–æ•°æ®

```python
import request

response = request.get('<http://www.baidu.com>')

# response.encoding = 'utf-8'
# print(response.txt)

print(response.content.decode()) # è‡ªåŠ¨å°†äºŒè¿›åˆ¶è½¬æ¢ä¸ºutf-8
```

## BeautifulSoup

> å¯ä»HTMLæˆ–XMLæ–‡ä»¶ä¸­æå–æ•°æ®çš„Pythonåº“

```python
pip install bs4
pip install lxml
```

### BeautifulSoupå¯¹è±¡

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup("<html>data</html>", 'lxml')
```

- BeautifulSoupå¯¹è±¡ï¼šä»£è¡¨è¦è§£ææ•´ä¸ªæ–‡æ¡£æ ‘
- æ”¯æŒéå†æ–‡æ¡£æ ‘å’Œæœç´¢æ–‡æ¡£æ ‘ä¸­æè¿°çš„å¤§éƒ¨åˆ†çš„æ–¹æ³•

### findæ–¹æ³•

```python
find(self, name=None, attrs={}, recursive=True, text=None, **kargs)
```

- nameï¼šæ ‡ç­¾å
- attrsï¼šå±æ€§å­—å…¸ï¼Œå¯ä»¥æŒ‡å®šæ ‡ç­¾çš„ç‰¹å®šå±æ€§æ¥è¿›è¡Œæœç´¢
- recursiveï¼šæ˜¯å¦é€’å½’æœç´¢ï¼Œé»˜è®¤ä¸ºTrue
- textï¼šæœç´¢åŒ…å«ç‰¹å®šæ–‡æœ¬å†…å®¹çš„æ ‡ç­¾
- returnï¼šè¿”å›æŸ¥æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªå…ƒç´ å¯¹è±¡

ğŸ’¡è‹¥è¦æŸ¥æ‰¾æŸä¸ªæ ‡ç­¾çš„å…¨éƒ¨å†…å®¹ï¼Œå¯ä»¥ä½¿ç”¨soup.find_all(name)

```python
# Tag å¯¹è±¡
print('æ ‡ç­¾å', a.name)
print('æ ‡ç­¾æ‰€æœ‰å±æ€§', a.attrs)
print('æ ‡ç­¾æ–‡æœ¬å†…å®¹', a.text)
```

## æ­£åˆ™è¡¨è¾¾å¼

> æ­£åˆ™è¡¨è¾¾å¼æ˜¯ä¸€ç§å­—ç¬¦ä¸²åŒ¹é…çš„æ¨¡å¼

ä½œç”¨ï¼š

- æ£€æŸ¥ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦å«æœ‰æŸç§å­ä¸²
- æ›¿æ¢åŒ¹é…çš„å­ä¸²
- æå–æŸä¸ªå­—ç¬¦ä¸²ä¸­åŒ¹é…çš„å­ä¸²

```python
import re

rs = re.findall('abc', 'abcabc') # å‰é¢abcæ˜¯æ­£åˆ™è¡¨è¾¾å¼ï¼Œåé¢çš„æ˜¯éœ€è¦è¢«æŸ¥æ‰¾çš„å­—ç¬¦ä¸²
rs = re.findall('a.c', 'abc') # å¯ä»¥åŒ¹é…é™¤æ¢è¡Œç¬¦ä»¥å¤–çš„å­—ç¬¦
rs = re.findall('a\\.c', 'a.c') # è®©'.'è½¬æ¢ä¸ºæ™®é€šçš„å­—ç¬¦è€Œéè½¬ä¹‰å­—ç¬¦
rs = re.findall('a[bc]d', 'abd') # || â€˜æˆ–â€™å…³ç³»
```

## re.findall()æ–¹æ³•

API

```python
re.findall(pattern, string, flags=0)
```

å‚æ•°

- patternï¼šæ­£åˆ™è¡¨è¾¾å¼
- stringï¼šä»å“ªä¸ªå­—ç¬¦ä¸²ä¸­æŸ¥æ‰¾
- flagsï¼šåŒ¹é…æ¨¡å¼
  - re.DOTALL/Sï¼šâ€™.â€™åŒ¹é…ä»»æ„çš„å­—ç¬¦
  - re.IGNORECASE/Iï¼šå¿½ç•¥å¤§å°å†™è¿›è¡ŒåŒ¹é…
  - re.MULTILINE/Mï¼šå¤šè¡ŒåŒ¹é…ï¼Œå½±å“ ^ å’Œ $ ä½¿ ^ å’Œ $ åŒ¹é…æ¯ä¸€è¡Œçš„å¼€å§‹å’Œç»“æŸï¼Œè€Œä¸ä»…ä»…æ˜¯æ•´ä¸ªå­—ç¬¦ä¸²çš„å¼€å§‹å’Œç»“æŸ
  - re.VERBOSE/Xï¼šå…è®¸æ­£åˆ™è¡¨è¾¾å¼æ›´æ˜“è¯»ï¼Œå¿½ç•¥ç©ºç™½å’Œæ³¨é‡Š
  - re.ASCII/Aï¼šä½¿ \w, \W, \b, å’Œ \B åªåŒ¹é… ASCII å­—ç¬¦ï¼Œè€Œä¸åŒ¹é… Unicode å­—ç¬¦
  - re.LOCALE/Lï¼šæ ¹æ®å½“å‰åŒºåŸŸè®¾ç½®ï¼Œå½±å“\w, \W, \b, å’Œ \Bçš„åŒ¹é…è¡Œä¸º
- returnï¼šæ‰«ææ•´ä¸ªstringå­—ç¬¦ä¸²ï¼Œè¿”å›æ‰€æœ‰ä¸patternåŒ¹é…çš„åˆ—è¡¨

ğŸ’¡re.findall(â€a(.+)bcâ€, â€œa\nbcâ€, re.DOTALL)çš„è¿”å›ç»“æœæ˜¯â€\nâ€ï¼Œå› ä¸ºè¿™é‡Œä½¿ç”¨äº†()æ¥è¿›è¡Œåˆ†ç»„

## råŸä¸²çš„ä½¿ç”¨

```python
rs = re.findall('a\\\\nbc', 'a\\\\nbc')
[out]:[]

rs = re.findall(r'a\\\\nbc', 'a\\\\nbc')
[out]:['a\\\\nbc']
```

 ğŸ’¡æ‹“å±•ï¼šå¯ä»¥è§£å†³å†™æ­£åˆ™çš„æ—¶å€™ï¼Œä¸ç¬¦åˆPEP8è§„èŒƒçš„é—®é¢˜

## å®ä¾‹

```python
import requests
from bs4 import BeautifulSoup
import re

response = requests.get('http://ncov.dxy.cn/ncovh5/view/pneumoniaj')
page = response.content.decode()

soup = BeautifulSoup(page, 'lxml')
script = soup.find(id='getListByCountryTypeService2true')

countries_text = script.text

json_str = re.findall(r'(\[.*\])', countries_text)
```

