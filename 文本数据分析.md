# æ–‡æœ¬æ•°æ®åˆ†æ

> æ–‡æœ¬æ•°æ®åˆ†æèƒ½å¤Ÿæœ‰æ•ˆå¸®åŠ©æˆ‘ä»¬ç†è§£æ•°æ®è¯­æ–™ï¼Œå¿«é€Ÿæ£€æŸ¥å‡ºè¯­æ–™å¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼Œå¹¶æŒ‡å¯¼ä¹‹åæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¸€äº›è¶…å‚æ•°çš„é€‰æ‹©

- å¸¸ç”¨çš„å‡ ç§æ–‡æœ¬æ•°æ®åˆ†ææ–¹æ³•
  - æ ‡ç­¾æ•°é‡åˆ†å¸ƒ
  - å¥å­é•¿åº¦åˆ†å¸ƒ
  - è¯é¢‘ç»Ÿè®¡ä¸å…³é”®è¯è¯äº‘

<aside> ğŸ’¡

å°†åŸºäºä¸­æ–‡é…’åº—è¯„è®ºè¯­æ–™æ¥è®²è§£å¸¸ç”¨çš„å‡ ç§æ–‡æœ¬æ•°æ®åˆ†ææ–¹æ³•

å±äºäºŒåˆ†ç±»çš„ä¸­æ–‡æƒ…æ„Ÿåˆ†æè¯­æ–™ï¼Œdev.tsvä¸ºéªŒè¯æ•°æ®é›†

</aside>

## æ ‡ç­¾æ•°é‡åˆ†å¸ƒ

- è·å¾—è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¹¶å¯è§†åŒ–å…¶æ ‡ç­¾åˆ†å¸ƒ

```python
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

# æ˜¾ç¤ºé£æ ¼
plt.style.use('fivethirtyeight')

# åˆ†åˆ«è¯»å–è®­ç»ƒå’ŒéªŒè¯tsv
train_data = pd.read_csv("D:/dev/python/pyWork/NLP/data/cn_data/train.tsv", sep="\\t")
val_data = pd.read_csv("D:/dev/python/pyWork/NLP/data/cn_data/dev.tsv", sep="\\t")

# è·å¾—è®­ç»ƒæ•°æ®é›†æ ‡ç­¾æ•°é‡åˆ†å¸ƒ
sns.countplot(x="label", data=train_data)
plt.title("train_data")
plt.show()

# è·å–éªŒè¯æ•°æ®æ ‡ç­¾æ•°é‡åˆ†å¸ƒ
sns.countplot(x="label", data=val_data)
plt.title("val_data")
plt.show()
```

<aside> ğŸ’¡

åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨ACCä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œè‹¥æƒ³å°†ACCçš„åŸºçº¿å®šä¹‰åœ¨50%å·¦å³ï¼Œåˆ™éœ€è¦æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ç»´æŒåœ¨1:1å·¦å³ï¼Œå¦åˆ™å°±è¦è¿›è¡Œå¿…è¦çš„æ•°æ®å¢å¼ºæˆ–æ•°æ®åˆ å‡

</aside>

## å¥å­é•¿åº¦åˆ†å¸ƒ

- è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å¥å­é•¿åº¦åˆ†å¸ƒ

```python
# åœ¨è®­ç»ƒæ•°æ®ä¸­æ·»åŠ æ–°çš„å¥å­é•¿åº¦åˆ—ï¼Œæ¯ä¸ªå…ƒç´ çš„å€¼éƒ½æ˜¯å¯¹åº”å¥å­çš„é•¿åº¦
train_data["sentence_length"] = list(map(lambda x: len(x), train_data["sentence"]))

# ç»˜åˆ¶å¥å­é•¿åº¦åˆ—çš„æ•°é‡åˆ†å¸ƒå›¾
sns.displot(train_data["sentence_length"])

# ä¸»è¦å…³æ³¨disté•¿åº¦åˆ†å¸ƒæ¨ªåæ ‡ï¼Œä¸éœ€è¦ç»˜åˆ¶çºµåæ ‡
plt.yticks([])
plt.show()

# åœ¨éªŒè¯æ•°æ®ä¸­æ·»åŠ æ–°çš„å¥å­é•¿åº¦åˆ—ï¼Œæ¯ä¸ªå…ƒç´ çš„å€¼éƒ½æ˜¯å¯¹åº”å¥å­çš„é•¿åº¦
val_data["sentence_length"] = list(map(lambda x: len(x), val_data["sentence"]))

# ç»˜åˆ¶
sns.displot(val_data["sentence_length"])

# ä¸»è¦å…³æ³¨counté•¿åº¦åˆ†å¸ƒçš„çºµåæ ‡ï¼Œä¸éœ€è¦ç»˜åˆ¶æ¨ªåæ ‡ï¼Œæ¨ªåæ ‡èŒƒå›´é€šè¿‡distå›¾è¿›è¡ŒæŸ¥çœ‹
plt.xticks([])
plt.show()

# ç»˜åˆ¶disté•¿åº¦åˆ†å¸ƒå›¾
sns.displot(val_data["sentence_length"])

plt.yticks([])
plt.show()
```

- åˆ†æï¼šé€šè¿‡ç»˜åˆ¶å¥å­é•¿åº¦åˆ†å¸ƒå›¾ï¼Œå¯ä»¥å¾—çŸ¥æˆ‘ä»¬çš„è¯­æ–™ä¸­å¤§éƒ¨åˆ†å¥å­é•¿åº¦çš„åˆ†å¸ƒèŒƒå›´ï¼Œå› ä¸ºæ¨¡å‹çš„è¾“å…¥è¦æ±‚ä¸ºå›ºå®šå°ºå¯¸çš„å¼ é‡ï¼Œåˆç†çš„é•¿åº¦èŒƒå›´å¯¹ä¹‹åè¿›è¡Œå¥å­æˆªæ–­è¡¥é½èµ·åˆ°å…³é”®ä½œç”¨

```python
# ç»˜åˆ¶æ•£ç‚¹åˆ†å¸ƒå›¾
sns.stripplot(y='sentence_length', x='label', data=train_data)
plt.show()
```

- åˆ†æï¼šé€šè¿‡æŸ¥çœ‹æ­£è´Ÿæ ·æœ¬é•¿åº¦æ•£ç‚¹å›¾ï¼Œå¯ä»¥æœ‰æ•ˆå®šä½å¼‚å¸¸ç‚¹å‡ºç°çš„ä½ç½®ï¼Œä»¥åŠ©æ›´ç²¾ç¡®çš„äººå·¥è¯­æ–™å®¡æŸ¥

## è¯æ±‡æ€»æ•°ç»Ÿè®¡

```python
# è¿›è¡Œè®­ç»ƒé›†çš„å¥å­è¿›è¡Œåˆ†è¯ï¼Œå¹¶ç»Ÿè®¡å‡ºä¸åŒè¯æ±‡çš„æ€»æ•°
train_vocab = set(chain(*map(lambda x: jieba.lcut(x), train_data["sentence"])))
print("è®­ç»ƒé›†å…±åŒ…å«ä¸åŒè¯æ±‡æ€»æ•°ä¸º:", len(train_vocab))

# è¿›è¡ŒéªŒè¯é›†çš„å¥å­è¿›è¡Œåˆ†è¯ï¼Œå¹¶ç»Ÿè®¡å‡ºä¸åŒè¯æ±‡çš„æ€»æ•°
val_vocab = set(chain(*map(lambda x: jieba.lcut(x), val_data["sentence"])))
print("è®­ç»ƒé›†å…±åŒ…å«ä¸åŒè¯æ±‡æ€»æ•°ä¸º:", len(val_vocab))
```

## å…³é”®è¯è¯äº‘

![è¯äº‘](https://prod-files-secure.s3.us-west-2.amazonaws.com/47b2d2d9-076d-4608-a4c7-2dafaff3bcf4/caef1bd5-b733-4cdb-820f-61455cb3b8b8/image.png)

è¯äº‘

```python
# ä½¿ç”¨jiebaä¸­çš„è¯æ€§æ ‡æ³¨åŠŸèƒ½
import jieba.posseg as pseg
import matplotlib.pyplot as plt
from itertools import chain
import pandas as pd

def get_a_list(text):
    """ç”¨äºè·å–å½¢å®¹è¯åˆ—è¡¨"""
    # ä½¿ç”¨jiebaçš„è¯æ€§æ ‡æ³¨æ–¹æ³•åˆ‡åˆ†æ–‡æœ¬ï¼Œè·å¾—å…·æœ‰è¯æ€§å±æ€§flagå’Œè¯æ±‡å±æ€§wordçš„å¯¹è±¡ï¼Œ
    # ä»è€Œåˆ¤æ–­flagæ˜¯å¦ä¸ºå½¢å®¹è¯ï¼Œè¿”å›å¯¹åº”çš„è¯æ±‡
    r = []
    for g in pseg.lcut(text):
        if g.flag == "a":
            r.append(g.word)
    return r

# å¯¼å…¥ç»˜åˆ¶è¯äº‘çš„å·¥å…·åŒ…
from wordcloud import WordCloud

def get_word_cloud(keywords_list):
    # å®ä¾‹åŒ–ç»˜åˆ¶è¯äº‘çš„ç±»ï¼Œå…¶ä¸­å‚æ•°font_pathæ˜¯å­—ä½“è·¯å¾„ï¼Œä¸ºäº†èƒ½å¤Ÿæ˜¾ç¤ºä¸­æ–‡ï¼Œ
    # max_wordsæŒ‡è¯äº‘å›¾åƒæœ€å¤šæ˜¾ç¤ºå¤šå°‘ä¸ªè¯ï¼Œbackground_colorä¸ºèƒŒæ™¯é¢œè‰²
    wordcloud = WordCloud(font_path="C:/Windows/Fonts/SimHei.ttf", max_words=100, background_color="white")
    # å°†ä¼ å…¥çš„åˆ—è¡¨è½¬æ¢ä¸ºè¯äº‘ç”Ÿæˆå™¨éœ€è¦çš„å­—ç¬¦ä¸²å½¢å¼
    keywords_string = " ".join(keywords_list)
    # ç”Ÿæˆè¯äº‘
    wordcloud.generate(keywords_string)

    # ç»˜åˆ¶å›¾åƒå¹¶æ˜¾ç¤º
    plt.figure()
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

train_data = pd.read_csv("D:/dev/python/pyWork/NLP/data/cn_data/train.tsv", sep="\\t")

# è·å¾—è®­ç»ƒé›†ä¸Šæ­£æ ·æœ¬
p_train_data = train_data[train_data["label"] == 1]["sentence"]

# å¯¹æ­£æ ·æœ¬çš„æ¯ä¸ªå¥å­çš„å½¢å®¹è¯
train_p_a_vocab = chain(*map(lambda x: get_a_list(x), p_train_data))
# print(train_p_n_vocab)

# è·å¾—è®­ç»ƒé›†ä¸Šè´Ÿæ ·æœ¬
n_train_data = train_data[train_data["label"] == 0]["sentence"]

# å¯¹è´Ÿæ ·æœ¬çš„æ¯ä¸ªå¥å­çš„å½¢å®¹è¯
train_n_a_vocab = chain(*map(lambda x: get_a_list(x), n_train_data))

# è°ƒç”¨ç»˜åˆ¶è¯äº‘å‡½æ•°
get_word_cloud(train_p_a_vocab)
get_word_cloud(train_n_a_vocab)
```