# 文本数据分析

> 文本数据分析能够有效帮助我们理解数据语料，快速检查出语料可能存在的问题，并指导之后模型训练过程中一些超参数的选择

- 常用的几种文本数据分析方法
  - 标签数量分布
  - 句子长度分布
  - 词频统计与关键词词云

<aside> 💡

将基于中文酒店评论语料来讲解常用的几种文本数据分析方法

属于二分类的中文情感分析语料，dev.tsv为验证数据集

</aside>

## 标签数量分布

- 获得训练集和验证集，并可视化其标签分布

```python
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

# 显示风格
plt.style.use('fivethirtyeight')

# 分别读取训练和验证tsv
train_data = pd.read_csv("D:/dev/python/pyWork/NLP/data/cn_data/train.tsv", sep="\\t")
val_data = pd.read_csv("D:/dev/python/pyWork/NLP/data/cn_data/dev.tsv", sep="\\t")

# 获得训练数据集标签数量分布
sns.countplot(x="label", data=train_data)
plt.title("train_data")
plt.show()

# 获取验证数据标签数量分布
sns.countplot(x="label", data=val_data)
plt.title("val_data")
plt.show()
```

<aside> 💡

在深度学习模型评估中，我们一般使用ACC作为评估指标，若想将ACC的基线定义在50%左右，则需要正负样本比例维持在1:1左右，否则就要进行必要的数据增强或数据删减

</aside>

## 句子长度分布

- 获取训练集和验证集的句子长度分布

```python
# 在训练数据中添加新的句子长度列，每个元素的值都是对应句子的长度
train_data["sentence_length"] = list(map(lambda x: len(x), train_data["sentence"]))

# 绘制句子长度列的数量分布图
sns.displot(train_data["sentence_length"])

# 主要关注dist长度分布横坐标，不需要绘制纵坐标
plt.yticks([])
plt.show()

# 在验证数据中添加新的句子长度列，每个元素的值都是对应句子的长度
val_data["sentence_length"] = list(map(lambda x: len(x), val_data["sentence"]))

# 绘制
sns.displot(val_data["sentence_length"])

# 主要关注count长度分布的纵坐标，不需要绘制横坐标，横坐标范围通过dist图进行查看
plt.xticks([])
plt.show()

# 绘制dist长度分布图
sns.displot(val_data["sentence_length"])

plt.yticks([])
plt.show()
```

- 分析：通过绘制句子长度分布图，可以得知我们的语料中大部分句子长度的分布范围，因为模型的输入要求为固定尺寸的张量，合理的长度范围对之后进行句子截断补齐起到关键作用

```python
# 绘制散点分布图
sns.stripplot(y='sentence_length', x='label', data=train_data)
plt.show()
```

- 分析：通过查看正负样本长度散点图，可以有效定位异常点出现的位置，以助更精确的人工语料审查

## 词汇总数统计

```python
# 进行训练集的句子进行分词，并统计出不同词汇的总数
train_vocab = set(chain(*map(lambda x: jieba.lcut(x), train_data["sentence"])))
print("训练集共包含不同词汇总数为:", len(train_vocab))

# 进行验证集的句子进行分词，并统计出不同词汇的总数
val_vocab = set(chain(*map(lambda x: jieba.lcut(x), val_data["sentence"])))
print("训练集共包含不同词汇总数为:", len(val_vocab))
```

## 关键词词云

![词云](https://prod-files-secure.s3.us-west-2.amazonaws.com/47b2d2d9-076d-4608-a4c7-2dafaff3bcf4/caef1bd5-b733-4cdb-820f-61455cb3b8b8/image.png)

词云

```python
# 使用jieba中的词性标注功能
import jieba.posseg as pseg
import matplotlib.pyplot as plt
from itertools import chain
import pandas as pd

def get_a_list(text):
    """用于获取形容词列表"""
    # 使用jieba的词性标注方法切分文本，获得具有词性属性flag和词汇属性word的对象，
    # 从而判断flag是否为形容词，返回对应的词汇
    r = []
    for g in pseg.lcut(text):
        if g.flag == "a":
            r.append(g.word)
    return r

# 导入绘制词云的工具包
from wordcloud import WordCloud

def get_word_cloud(keywords_list):
    # 实例化绘制词云的类，其中参数font_path是字体路径，为了能够显示中文，
    # max_words指词云图像最多显示多少个词，background_color为背景颜色
    wordcloud = WordCloud(font_path="C:/Windows/Fonts/SimHei.ttf", max_words=100, background_color="white")
    # 将传入的列表转换为词云生成器需要的字符串形式
    keywords_string = " ".join(keywords_list)
    # 生成词云
    wordcloud.generate(keywords_string)

    # 绘制图像并显示
    plt.figure()
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

train_data = pd.read_csv("D:/dev/python/pyWork/NLP/data/cn_data/train.tsv", sep="\\t")

# 获得训练集上正样本
p_train_data = train_data[train_data["label"] == 1]["sentence"]

# 对正样本的每个句子的形容词
train_p_a_vocab = chain(*map(lambda x: get_a_list(x), p_train_data))
# print(train_p_n_vocab)

# 获得训练集上负样本
n_train_data = train_data[train_data["label"] == 0]["sentence"]

# 对负样本的每个句子的形容词
train_n_a_vocab = chain(*map(lambda x: get_a_list(x), n_train_data))

# 调用绘制词云函数
get_word_cloud(train_p_a_vocab)
get_word_cloud(train_n_a_vocab)
```